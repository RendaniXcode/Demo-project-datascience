# Module 1: Data Acquisition & Exploration

This module focuses on the first steps of any data science project: acquiring data from various sources and performing initial exploratory analysis.

## Learning Objectives

By the end of this module, you will be able to:
- Collect data from different sources (APIs, web scraping, databases, files)
- Perform exploratory data analysis (EDA)
- Create meaningful visualizations to understand data patterns
- Identify potential issues in datasets
- Document your findings effectively

## Contents

- **Notebooks**: Jupyter notebooks demonstrating various techniques
- **src**: Reusable Python modules for data acquisition and exploration
- **data**: Sample datasets and acquired data
- **results**: Outputs from exploration and analysis

## Projects

1. **Web API Data Collection**
   - Accessing public APIs
   - Handling API authentication
   - Processing JSON responses

2. **Web Scraping Basics**
   - Ethical web scraping practices
   - Using BeautifulSoup and Scrapy
   - Handling dynamic content with Selenium

3. **Database Connections**
   - SQL database interactions
   - NoSQL database basics
   - Efficient data retrieval

4. **Exploratory Data Analysis**
   - Statistical summaries
   - Data visualization
   - Pattern identification
   - Correlation analysis

## Getting Started

1. Review the notebooks in the `notebooks` directory
2. Run the examples with your own datasets
3. Complete the exercises at the end of each notebook

## Prerequisites

- Python 3.8+
- Basic understanding of HTTP requests
- Familiarity with pandas

## Resources

- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Requests Library](https://docs.python-requests.org/en/latest/)
- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)
- [Seaborn Documentation](https://seaborn.pydata.org/)